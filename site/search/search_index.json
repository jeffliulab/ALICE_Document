{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"A.L.I.C.E. PROJECT Document","text":""},{"location":"Development_Hisotry/V1_Dev_Diary/","title":"V1 Development Log (Diary)","text":""},{"location":"Development_Hisotry/V1_Dev_Diary/#v10-mvp-demo","title":"V1.0 MVP DEMO","text":""},{"location":"Development_Hisotry/V1_Dev_Diary/#june-5-2025","title":"June 5, 2025","text":"<p>Download Godot, create my first Godot project.</p> <p>Learn how to create a map, how to use tile and tilemap.</p> <p></p>"},{"location":"Development_Hisotry/V1_Dev_Diary/#june-6-2025","title":"June 6, 2025","text":"<p>Learn how to create conversations.</p> <p>Use Ollama and download <code>llama3.1:8b-instruct-q4_K_M</code> as the demo's inference model.</p> <p></p>"},{"location":"Development_Hisotry/V1_Dev_Diary/#june-7-2025","title":"June 7, 2025","text":"<p>Create Resident Class and implement two characters as instances.</p> <p>Test different versions, it can somehow communicate but the conversation (use json format) is HIGHLY UNSTABLE. The problem mainly happens in the format of json, I think it might because the llama3.1:8b-instruct-q4_K_M doesn't have enought inference power on json format output.</p> <p>See test_scripts/V1/test4</p> <p></p>"}]}